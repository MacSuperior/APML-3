{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 – Topic Modeling and Clustering for Online Social Media Data\n",
    "\n",
    "*Due: Friday January 12 at 14:00 CET*\n",
    "\n",
    "In the third assignment of the course Applications of Machine Learning (INFOB3APML), you will learn to use topic modeling and clustering to identify topics in online social media data. The objectives of this assignment are:\n",
    "- understand and process the text data\n",
    "- use the clustering algorithm to determine clusters in real-life data\n",
    "- use the Latent Dirichlet Allocation algorithm to identify discussed topics in real-life text data \n",
    "- use the visualization tools to validate the results of unsupervised learning and interpret your findings\n",
    "- reflect on the difference between two type of unsupervised learning algorithms\n",
    "\n",
    "In this assignment, you are going to discover the different ‘topics’ from a real social media text dataset. The project is divided into two parts (4 subtasks):\n",
    "\n",
    "- The first part contains data processing (1.1) and feature extraction (1.2) from the raw text data.\n",
    "- In the second part, you will implement two methods (2.1), a topic modeling method and a clustering method, to identify topics from the processed data. Then, the evaluation will be done by using visualization tools (2.2). \n",
    "\n",
    "Provided files:\n",
    "- The dataset: data/raw_data.txt\n",
    "- A tutorial notebook showcases some packages you could use for this assignment (optional): Ass3_tutorial.ipynb\n",
    "- Some sample visualization codes for interpreting the topic results: viz_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from emoji import demojize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, Birch\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Dataset:\n",
    " The data used in this assignment is Dutch text data. We collected the COVID-19 crisis related messages from online social media (Twitter) from January to November 2021. Then, a subset of raw tweets was randomly sampled. In total, our dataset includes the text data of about 100K messages. **To protect the data privacy, please only use this dataset within the course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "\n",
    "def phase0_open_txt_stream(filename):\n",
    "    return io.open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "pipe = phase0_open_txt_stream(\"others/data/raw_data.txt\")\n",
    "\n",
    "all_messages = []\n",
    "\n",
    "for message in pipe:\n",
    "    all_messages.append(message.strip())\n",
    "\n",
    "pipe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(all_messages)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Before you start the Project: \n",
    " The provided messages in the raw dataset were collected based on 10 different themes that relate to the COVID-19 crisis. Here is a list of all themes:\n",
    " -\tLockdown\n",
    " -\tFace mask\n",
    " -\tSocial distancing\n",
    " -\tLoneliness\n",
    " -\tHappiness\n",
    " -\tVaccine\n",
    " -\tTesting\n",
    " -  Curfew\n",
    " -  Covid entry pass\n",
    " -  Work from home\n",
    "\n",
    "Before starting your project, you need to first filter the messages (all messages are in Dutch) and use the messages belonging to only one theme for the topic identification. \n",
    " \n",
    "If you have submitted the theme preference, you can skip the following paragraph.\n",
    "\n",
    "*Please notice that there will be maximum two teams working on a same theme. In this way, we hope that each group will develop their own dataset and come up with interesting results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Data Processing\n",
    " In the first part of the assignment, please first filter the messages and use the messages belonging to your allocated theme for the identification of topics. For that you will need to:\n",
    " -\tDesign your query (e.g. a regular expression or a set of keywords) and filter the related messages for your allocated theme. \n",
    " -\tClean your filtered messages and preprocess them into the right representation. Please refer to the text data pre-processing and representation methods discussed in the lecture. You may use some of the recommended packages for text data preprocessing and representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter the related messages\n",
    "facemask_messages = []\n",
    "\n",
    "facemask_pattern = r\"mondkap|(mond)?masker|bekluier|beklap|\\bmask|muilkorf|facemask|face mask|monddoek|face shield|gelaats?scherm|gezichtsmasker|muilmasker|gezichtsbeschermingskapje|gezichtsschild|kap|eendenbek|gelaatsmasker\"\n",
    "for message in all_messages:\n",
    "    if re.search(facemask_pattern, message.lower()):\n",
    "        facemask_messages.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(facemask_messages)} messages about facemasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean and preprocess the messages\n",
    "\n",
    "def lowercase_doc(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Transform a document into lowercase.\n",
    "    \"\"\"\n",
    "    return doc.lower()\n",
    "\n",
    "\n",
    "def remove_urls(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Return a document stripped of any URLs.\n",
    "    \"\"\"\n",
    "    url_pattern = r\"http[s]*\\S+\"\n",
    "    clean_doc = re.sub(url_pattern, ' ', doc)\n",
    "    return clean_doc\n",
    "\n",
    "\n",
    "def remove_punctuation(doc: str) -> str: \n",
    "    \"\"\"\n",
    "    Transform a text document into a document without punctuation, digits, newline characters and twitter tags (@HugodeJonge, etc.).\n",
    "    \"\"\"\n",
    "    doc = doc.replace('\\\\n', ' ')\n",
    "\n",
    "    punct_pattern = r\"@\\w+\\b|[^\\w\\s]|\\d+|\\brt\\b\"\n",
    "    \n",
    "    clean_doc = re.sub(punct_pattern, ' ', doc, )\n",
    "\n",
    "    return clean_doc\n",
    "    \n",
    "\n",
    "def tokenize(doc: str, nlp, lemma = True) -> list:\n",
    "    \"\"\"\n",
    "    Transform string document into a tokenized list.\n",
    "    The lemma parameter specifiec whether to return lemmatized words or not.\n",
    "    \"\"\"\n",
    "    if lemma:\n",
    "        return [token.lemma_ for token in nlp(doc) if not token.is_stop]\n",
    "    else:\n",
    "        return [token for token in nlp(doc) if not token.is_stop]\n",
    "\n",
    "def convert_mondkapje_to_mondkap(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert every occurence of 'mondkapje' into 'mondkap'.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.replace('mondkapje', 'mondkap')\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "cleaned_messages = []\n",
    "\n",
    "for sentence in tqdm(facemask_messages):\n",
    "\n",
    "    sentence = lowercase_doc(sentence)\n",
    "    sentence = demojize(sentence)\n",
    "    sentence = remove_urls(sentence)\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    tokenized = tokenize(sentence, nlp)\n",
    "    cleaned_message = ' '.join(tokenized)\n",
    "    cleaned_message = convert_mondkapje_to_mondkap(cleaned_message)\n",
    "\n",
    "    cleaned_messages.append(cleaned_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for before, after in zip(facemask_messages[0:5], cleaned_messages[0:5]):\n",
    "    print(f\"Before cleaning:\\n{before}\\nAfter cleaning:\\n{after}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: represent the messages into formats that can be used in clustering or LDA algorithms (you may need different represention for two algorithms)\n",
    "\n",
    "# LDA input\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "lda_input = vectorizer.fit_transform(cleaned_messages)\n",
    "lda_featurenames = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering input\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized = vectorizer.fit_transform(cleaned_messages)\n",
    "clustering_input = vectorized.toarray()\n",
    "clustering_featurename = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.2 Exploratory Data Analysis\n",
    " After preprocessing the data, create at least 2 figures or tables that help you understand the data.\n",
    "\n",
    " While exploring the data, you may also think about questions such as:\n",
    " - Can you spot any differences between Twitter data and usual text data?\n",
    " - Does your exploration reveal some issues that would make it difficult to interpret the topics?\n",
    " - Can you improve the data by adding additional preprocessing steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot figure(s)\n",
    "\n",
    "# WORDCLOUD\n",
    "\n",
    "facemask_image = np.array(Image.open(\"mondkapplaatje.jpg\"))\n",
    "\n",
    "wordcloud_messages = ' '.join(cleaned_messages)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', mask = facemask_image).generate(wordcloud_messages)\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "for tweet in facemask_messages:\n",
    "    hashtags += re.findall(r'#(\\w+)', tweet)\n",
    "    \n",
    "hashtag_frequence = Counter(hashtags)\n",
    "\n",
    "hashtags_sorted = hashtag_frequence.most_common(5)\n",
    "hashtag_x, hashtag_y = zip(*hashtags_sorted)\n",
    "\n",
    "plt.figure(figsize=[10, 10])\n",
    "plt.bar(hashtag_x, hashtag_y, color='darkslategrey')\n",
    "plt.xlabel('Most occurring hashtags')\n",
    "plt.ylabel('Frequency per hashtag')\n",
    "plt.title('Top 5 most commonly used hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Topic modelling and clustering\n",
    " In the second part of the assignment, you will first:\n",
    " -\tImplement a Latent Dirichlet Allocation (LDA) algorithm to identify the discussed topics for your theme\n",
    " -\tImplement a clustering method  to cluster messages into different groups, then represent the topic of each cluster using a bag of words\n",
    "\n",
    "While implementing the algorithms, you may use the codes from the recommended packages. In the final report, please explain reasons to select the used algorithm/package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENSIM\n",
    "words = [message.split() for message in cleaned_messages]\n",
    "id2word = gensim.corpora.Dictionary(words)\n",
    "corpus = [id2word.doc2bow(word) for word in words]\n",
    "\n",
    "gensim_lda = gensim.models.LdaMulticore(\n",
    "    num_topics = 5,\n",
    "    corpus = corpus,\n",
    "    id2word = id2word\n",
    ")\n",
    "\n",
    "gensim_lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cluster the messages using a clustering algorithm\n",
    "# TODO: cluster the messages using a clustering algorithm\n",
    "k = 6\n",
    "\n",
    "# Principal Component Analysis for a better visualization\n",
    "pca = PCA(n_components = 2)\n",
    "visualization_data = pca.fit_transform(clustering_input)\n",
    "\n",
    "kmeans = KMeans(n_clusters = k, random_state = 9)\n",
    "kmeans.fit(clustering_input)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(k):\n",
    "    plt.scatter(\n",
    "        visualization_data[labels == i, 0],\n",
    "        visualization_data[labels == i, 1]\n",
    "    )\n",
    "    \n",
    "plt.title(\"Text clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_cluster = []\n",
    "\n",
    "for label in range(k):\n",
    "    indices = np.where(labels == label)[0]\n",
    "\n",
    "    cluster = np.array(cleaned_messages)[indices]\n",
    "\n",
    "    all_cluster_words = []\n",
    "    for sentence in cluster:\n",
    "        all_cluster_words += sentence.split()\n",
    "    \n",
    "    words_per_cluster.append(all_cluster_words)\n",
    "\n",
    "words_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_words in words_per_cluster:\n",
    "\n",
    "    wordcloud_messages = ' '.join(cluster_words)\n",
    "    wordcloud = WordCloud(width=600, height=600, background_color='white').generate(wordcloud_messages)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.2 Results, evaluation and Interpretation \n",
    " \n",
    "Finally, you will describe, evaluate and interpret your findings from two methods. \n",
    "\n",
    "- In the report, you need to describe and discuss the similarity and difference of results from two methods.\n",
    "- While evaluating the results, human judgment is very important, so visualization techniques are helpful to evaluate the identified topics in an interpreted manner. \n",
    "    \n",
    "1. For evaluating the topic modelling algorithm, please first use the interactive tool **[pyLDAvis](https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb#topic=0&lambda=1&term=)** to examine the inter-topic separation of your findings. \n",
    "\n",
    "2. For interpreting the identified topics / clusters of both algorithms, we provide example code for several visualization techiques. You can use multiple ones to evaluate your results or come up with visualisations on your own. The files contain examples for how to use the visualisation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluation \n",
    "# Transform the gensim LDA model for visualization\n",
    "lda_display = gensimvis.prepare(gensim_lda, corpus, id2word, sort_topics=False)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(lda_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus task. For each task that is successfully completed, you may obtain max. 1 extra point. \n",
    "\n",
    "1. Implement another clustering algorithm or design your own clustering algorithm. Discuss your findings and explain why this is a better (or worse) clustering algorithm than the above one (the clustering algorithm, not LDA).\n",
    "\n",
    "2. Can you think of other evaluation methods than the provided visualization techniques? If so, implement one and explain why it is a good evaluation for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER CLUSTERING ALGORITHM\n",
    "\n",
    "# TODO: cluster the messages using a clustering algorithm\n",
    "k = 6\n",
    "\n",
    "# Principal Component Analysis for a better visualization\n",
    "pca = PCA(n_components = 2)\n",
    "visualization_data = pca.fit_transform(clustering_input)\n",
    "\n",
    "kmeans = Birch(n_clusters = 6)\n",
    "kmeans.fit(clustering_input)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(k):\n",
    "    plt.scatter(\n",
    "        visualization_data[labels == i, 0],\n",
    "        visualization_data[labels == i, 1]\n",
    "    )\n",
    "    \n",
    "plt.title(\"Text clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_cluster = []\n",
    "\n",
    "for label in range(k):\n",
    "    indices = np.where(labels == label)[0]\n",
    "\n",
    "    cluster = np.array(cleaned_messages)[indices]\n",
    "\n",
    "    all_cluster_words = []\n",
    "    for sentence in cluster:\n",
    "        all_cluster_words += sentence.split()\n",
    "    \n",
    "    words_per_cluster.append(all_cluster_words)\n",
    "\n",
    "words_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_words in words_per_cluster:\n",
    "\n",
    "    wordcloud_messages = ' '.join(cluster_words)\n",
    "    wordcloud = WordCloud(width=600, height=600, background_color='white').generate(wordcloud_messages)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
